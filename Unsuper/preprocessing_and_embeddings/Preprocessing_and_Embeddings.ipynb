{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import sub\n",
    "import multiprocessing\n",
    "from unidecode import unidecode\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from time import time \n",
    "from collections import defaultdict\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv(\"../data.csv\")\n",
    "file_cleaned = file.dropna().drop_duplicates().reset_index(drop=True).rename(columns={'review':'title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    0.676095\n",
       "4.0    0.203063\n",
       "3.0    0.075295\n",
       "2.0    0.024383\n",
       "1.0    0.021165\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_cleaned.rating.value_counts()/len(file_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, rating]\n",
       "Index: []"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_cleaned[file_cleaned.rating==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_cleaned = file_cleaned[file_cleaned.rating!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    0.676095\n",
       "4.0    0.203063\n",
       "3.0    0.075295\n",
       "2.0    0.024383\n",
       "1.0    0.021165\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_cleaned.rating.value_counts()/len(file_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_list(text, remove_polish_letters):\n",
    "    ''' Pre process and convert texts to a list of words \n",
    "    method inspired by method from eliorc github repo: https://github.com/eliorc/Medium/blob/master/MaLSTM.ipynb'''\n",
    "    text = remove_polish_letters(text)\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = sub(r\"[^A-Za-z0-9^,!?.\\/'+]\", \" \", text)\n",
    "    text = sub(r\"\\+\", \" plus \", text)\n",
    "    text = sub(r\",\", \" \", text)\n",
    "    text = sub(r\"\\.\", \" \", text)\n",
    "    text = sub(r\"!\", \" ! \", text)\n",
    "    text = sub(r\"\\?\", \" ? \", text)\n",
    "    text = sub(r\"'\", \" \", text)\n",
    "    text = sub(r\":\", \" : \", text)\n",
    "    text = sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Remove all the special characters\n",
    "    sentence = sub(r'\\W', ' ', str(sen))\n",
    "\n",
    "    # remove all single characters\n",
    "    sentence= sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    sentence = sub(r'\\^[a-zA-Z]\\s+', ' ', sentence) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    sentence = sub(r'\\s+', ' ', sentence, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    sentence = sub(r'^b\\s+', '', sentence)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    sentence= sentence.lower()\n",
    "\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_cleaned.title = file_cleaned.title.apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model = file_cleaned.copy()\n",
    "file_model = file_model[file_model.title.str.len()>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 12:21:25: collecting all words and their counts\n",
      "INFO - 12:21:25: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 12:21:43: collected 1219 word types from a corpus of 4718372 words (unigram + bigrams) and 10253 sentences\n",
      "INFO - 12:21:43: using 1219 counts as vocab in Phrases<0 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 12:21:43: source_vocab length 1219\n",
      "INFO - 12:21:43: Phraser built with 0 phrasegrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the product does exactly as it should and is quite affordable did not realized it was double screened until it arrived so it was even better than had expected as an added bonus one of the screens carries small hint of the smell of an old grape candy used to buy so for reminiscent sake cannot stop putting the pop filter next to my nose and smelling it after recording dif you needed pop filter this will work just as well as the expensive ones and it may even come with pleasing aroma like mine did buy this product '"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = [row for row in file_model.title]\n",
    "phrases = Phrases(sent, min_count=1, progress_per=50000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- min count = 3 - remove most unusual words from training embeddings, like words 'ssssuuuuuuuppppppeeeeeerrrr', which actually stands for 'super', and doesn't need additional training\n",
    "- window = 4 - Word2Vec model will learn to predict given word from up to 4 words to the left, and up to 4 words to the right\n",
    "- size = 300 - size of hidden layer used to predict surroundings of embedded word, which also stands for dimensions of trained embeddings\n",
    "- sample = 1e-5 - probability baseline for subsampling most frequent words from surrounding of embedded word\n",
    "- negative = 20 - number of negative (ones that shouldn't have been predicted while modeling selected pair of words) words that will have their corresponding weights updated while training on specific training example, along with positive word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 12:21:53: collecting all words and their counts\n",
      "WARNING - 12:21:53: Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "INFO - 12:21:53: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 12:21:55: collected 38 word types from a corpus of 4718372 raw words and 10253 sentences\n",
      "INFO - 12:21:55: Loading a fresh vocabulary\n",
      "INFO - 12:21:55: effective_min_count=3 retains 38 unique words (100% of original 38, drops 0)\n",
      "INFO - 12:21:55: effective_min_count=3 leaves 4718372 word corpus (100% of original 4718372, drops 0)\n",
      "INFO - 12:21:55: deleting the raw counts dictionary of 38 items\n",
      "INFO - 12:21:55: sample=1e-05 downsamples 37 most-common words\n",
      "INFO - 12:21:55: downsampling leaves estimated 73296 word corpus (1.6% of prior 4718372)\n",
      "INFO - 12:21:55: estimated required memory for 38 words and 300 dimensions: 110200 bytes\n",
      "INFO - 12:21:55: resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.03 mins\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=3,\n",
    "                     window=4,\n",
    "                     size=300,\n",
    "                     sample=1e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "start = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=50000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 12:22:00: training model with 3 workers on 38 vocabulary and 300 features, using sg=0 hs=0 sample=1e-05 negative=20 window=4\n",
      "INFO - 12:22:01: EPOCH 1 - PROGRESS: at 64.23% examples, 42931 words/s, in_qsize 3, out_qsize 2\n",
      "INFO - 12:22:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:02: EPOCH - 1 : training on 4718372 raw words (72972 effective words) took 1.5s, 47098 effective words/s\n",
      "INFO - 12:22:03: EPOCH 2 - PROGRESS: at 87.46% examples, 62171 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 12:22:03: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:03: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:03: EPOCH - 2 : training on 4718372 raw words (74142 effective words) took 1.3s, 58995 effective words/s\n",
      "INFO - 12:22:04: EPOCH 3 - PROGRESS: at 87.66% examples, 61694 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 12:22:04: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:04: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:04: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:04: EPOCH - 3 : training on 4718372 raw words (73477 effective words) took 1.3s, 58398 effective words/s\n",
      "INFO - 12:22:05: EPOCH 4 - PROGRESS: at 84.47% examples, 59075 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 12:22:06: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:06: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:06: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:06: EPOCH - 4 : training on 4718372 raw words (73416 effective words) took 1.3s, 56372 effective words/s\n",
      "INFO - 12:22:07: EPOCH 5 - PROGRESS: at 64.46% examples, 43193 words/s, in_qsize 3, out_qsize 2\n",
      "INFO - 12:22:07: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:07: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:07: EPOCH - 5 : training on 4718372 raw words (73228 effective words) took 1.5s, 47522 effective words/s\n",
      "INFO - 12:22:08: EPOCH 6 - PROGRESS: at 86.20% examples, 60113 words/s, in_qsize 3, out_qsize 2\n",
      "INFO - 12:22:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:09: EPOCH - 6 : training on 4718372 raw words (72853 effective words) took 1.3s, 57498 effective words/s\n",
      "INFO - 12:22:10: EPOCH 7 - PROGRESS: at 83.85% examples, 58290 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 12:22:10: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:10: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:10: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:10: EPOCH - 7 : training on 4718372 raw words (73617 effective words) took 1.3s, 57326 effective words/s\n",
      "INFO - 12:22:11: EPOCH 8 - PROGRESS: at 86.20% examples, 60584 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 12:22:11: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:11: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:11: EPOCH - 8 : training on 4718372 raw words (73538 effective words) took 1.3s, 57470 effective words/s\n",
      "INFO - 12:22:12: EPOCH 9 - PROGRESS: at 62.18% examples, 41385 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 12:22:13: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:13: EPOCH - 9 : training on 4718372 raw words (73369 effective words) took 1.6s, 47114 effective words/s\n",
      "INFO - 12:22:14: EPOCH 10 - PROGRESS: at 86.33% examples, 60624 words/s, in_qsize 6, out_qsize 3\n",
      "INFO - 12:22:14: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:14: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:14: EPOCH - 10 : training on 4718372 raw words (73308 effective words) took 1.2s, 59852 effective words/s\n",
      "INFO - 12:22:15: EPOCH 11 - PROGRESS: at 89.06% examples, 62841 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 12:22:15: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:15: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:15: EPOCH - 11 : training on 4718372 raw words (73190 effective words) took 1.2s, 61267 effective words/s\n",
      "INFO - 12:22:16: EPOCH 12 - PROGRESS: at 86.94% examples, 60845 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 12:22:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:16: EPOCH - 12 : training on 4718372 raw words (72956 effective words) took 1.2s, 59551 effective words/s\n",
      "INFO - 12:22:17: EPOCH 13 - PROGRESS: at 73.25% examples, 49817 words/s, in_qsize 5, out_qsize 2\n",
      "INFO - 12:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:18: EPOCH - 13 : training on 4718372 raw words (72868 effective words) took 1.5s, 47300 effective words/s\n",
      "INFO - 12:22:19: EPOCH 14 - PROGRESS: at 89.19% examples, 63158 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 12:22:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:19: EPOCH - 14 : training on 4718372 raw words (73515 effective words) took 1.2s, 61613 effective words/s\n",
      "INFO - 12:22:20: EPOCH 15 - PROGRESS: at 87.46% examples, 61738 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 12:22:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:20: EPOCH - 15 : training on 4718372 raw words (73670 effective words) took 1.2s, 60211 effective words/s\n",
      "INFO - 12:22:21: EPOCH 16 - PROGRESS: at 86.90% examples, 60875 words/s, in_qsize 3, out_qsize 2\n",
      "INFO - 12:22:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:22: EPOCH - 16 : training on 4718372 raw words (73244 effective words) took 1.2s, 59961 effective words/s\n",
      "INFO - 12:22:23: EPOCH 17 - PROGRESS: at 84.39% examples, 59064 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 12:22:23: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:23: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:23: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:23: EPOCH - 17 : training on 4718372 raw words (73366 effective words) took 1.3s, 55794 effective words/s\n",
      "INFO - 12:22:24: EPOCH 18 - PROGRESS: at 65.15% examples, 44221 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 12:22:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:24: EPOCH - 18 : training on 4718372 raw words (73241 effective words) took 1.5s, 49630 effective words/s\n",
      "INFO - 12:22:25: EPOCH 19 - PROGRESS: at 89.59% examples, 63331 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 12:22:26: worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 12:22:26: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:26: EPOCH - 19 : training on 4718372 raw words (73178 effective words) took 1.2s, 61331 effective words/s\n",
      "INFO - 12:22:27: EPOCH 20 - PROGRESS: at 89.41% examples, 63653 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 12:22:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:27: EPOCH - 20 : training on 4718372 raw words (73486 effective words) took 1.2s, 61333 effective words/s\n",
      "INFO - 12:22:28: EPOCH 21 - PROGRESS: at 89.19% examples, 62818 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 12:22:28: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:28: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:28: EPOCH - 21 : training on 4718372 raw words (73158 effective words) took 1.2s, 61308 effective words/s\n",
      "INFO - 12:22:29: EPOCH 22 - PROGRESS: at 67.24% examples, 45017 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 12:22:30: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:30: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:30: EPOCH - 22 : training on 4718372 raw words (73030 effective words) took 1.6s, 45861 effective words/s\n",
      "INFO - 12:22:31: EPOCH 23 - PROGRESS: at 89.06% examples, 62744 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 12:22:31: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:31: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:31: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:31: EPOCH - 23 : training on 4718372 raw words (73356 effective words) took 1.2s, 61435 effective words/s\n",
      "INFO - 12:22:32: EPOCH 24 - PROGRESS: at 90.85% examples, 64590 words/s, in_qsize 6, out_qsize 3\n",
      "INFO - 12:22:32: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:32: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:32: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:32: EPOCH - 24 : training on 4718372 raw words (73311 effective words) took 1.2s, 63483 effective words/s\n",
      "INFO - 12:22:33: EPOCH 25 - PROGRESS: at 90.93% examples, 64364 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 12:22:33: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:33: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:33: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:33: EPOCH - 25 : training on 4718372 raw words (72805 effective words) took 1.2s, 59605 effective words/s\n",
      "INFO - 12:22:34: EPOCH 26 - PROGRESS: at 57.44% examples, 37720 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 12:22:35: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:35: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:35: EPOCH - 26 : training on 4718372 raw words (73006 effective words) took 1.9s, 38657 effective words/s\n",
      "INFO - 12:22:36: EPOCH 27 - PROGRESS: at 89.93% examples, 63826 words/s, in_qsize 3, out_qsize 2\n",
      "INFO - 12:22:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:36: EPOCH - 27 : training on 4718372 raw words (73339 effective words) took 1.2s, 62528 effective words/s\n",
      "INFO - 12:22:37: EPOCH 28 - PROGRESS: at 55.22% examples, 36395 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 12:22:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:38: EPOCH - 28 : training on 4718372 raw words (73471 effective words) took 1.7s, 44507 effective words/s\n",
      "INFO - 12:22:39: EPOCH 29 - PROGRESS: at 54.61% examples, 35596 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 12:22:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:40: EPOCH - 29 : training on 4718372 raw words (73501 effective words) took 2.0s, 37445 effective words/s\n",
      "INFO - 12:22:41: EPOCH 30 - PROGRESS: at 63.81% examples, 42775 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 12:22:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:42: EPOCH - 30 : training on 4718372 raw words (73479 effective words) took 1.6s, 47253 effective words/s\n",
      "INFO - 12:22:42: training on a 141551160 raw words (2199090 effective words) took 41.6s, 52912 effective words/s\n",
      "INFO - 12:22:42: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.69 mins\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - start) / 60, 2)))\n",
    "\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 12:22:52: saving Word2Vec object under word2vec4.model, separately None\n",
      "INFO - 12:22:52: not storing attribute vectors_norm\n",
      "INFO - 12:22:52: not storing attribute cum_table\n",
      "INFO - 12:22:52: saved word2vec4.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model.save(\"word2vec4.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting preprocessed dataset for further steps (with replaced bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_export = file_model.copy()\n",
    "file_export['old_title'] = file_export.title\n",
    "file_export.old_title = file_export.old_title.str.join(' ')\n",
    "file_export.title = file_export.title.apply(lambda x: ' '.join(bigram[x]))\n",
    "file_export.rating = file_export.rating.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_export[['title', 'rating']].to_csv('dataset4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
